#!/usr/bin/python3

import os
import json
import numpy as np
import random
import time
import sys
import statistics

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import deque, namedtuple
import pickle

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
import rospy
from std_msgs.msg import Float32MultiArray

from script.dqn.env import Env_DQN
from script.gail.env import Env_GAIL
from script.dqn.dqn_mem import ReplayMemory
from script.gail.gail import *
from script.gail.funcs import *


class DQN(nn.Module):
    # in = 30, out = 5
    def __init__(self, state_size, action_size):
        super().__init__()

        self.in_layer = nn.Linear(state_size, 64)
        self.h_layer = nn.Linear(64, 64)
        self.out_layer = nn.Linear(64, action_size)

        self.act = nn.ReLU()

    def forward(self, x):
        h1 = self.act(self.in_layer(x))
        h2 = self.act(self.h_layer(h1))
        output = self.out_layer(h2)
        return output


class Expert:
    def __init__(self, state_size, action_size):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # self.pub_result = rospy.Publisher("result", Float32MultiArray, queue_size=5)
        # self.result = Float32MultiArray()

        self.load_dqn = True
        self.load_file = "stage_30-10_policy_net_1000"
        self.memory_file = "memory1000"

        self.evaluation = False
        self.target_ckpt = "target_net.ckpt"
        self.target_mem = "memory1050.pkl"

        self.epoches = 50  # for train as you want
        self.steps = 200  ## 40e => 5000
        self.GAMMA = 0.99  # discount factor
        self.lr = 1e-4
        self.epsilon = 1.0
        self.epsilon_decay = 2000
        self.epsilon_min = 0.01
        self.epsilon_threshold = 1.0
        self.batch_size = 64  # can be modified as you want. upon your vram.
        self.train_start = 64
        self.TAU = 0.8

        self.env = Env_DQN(action_size)
        self.state_size = state_size
        self.action_size = action_size
        self.policy_net = DQN(state_size, action_size).to(self.device)
        self.target_net = DQN(state_size, action_size).to(self.device)
        self.optimizer = optim.AdamW(
            self.policy_net.parameters(), lr=self.lr, amsgrad=True
        )

        self.transition = namedtuple(
            "Transition", ("state", "action", "reward", "next_state")
        )
        self.memory = ReplayMemory(5000)

        self.is_random = False

    def getAction(self, state, global_step, load_episode):
        if self.evaluation == False:
            self.epsilon_threshold = self.epsilon_min + (
                self.epsilon - self.epsilon_min
            ) * np.exp(-1.0 * (global_step + load_episode) / self.epsilon_decay)

            if np.random.rand() > self.epsilon_threshold:
                self.is_random = False
                self.policy_net.eval()
                with torch.no_grad():
                    return self.policy_net(state).max(1)[1].view(1, 1)

            else:
                self.is_random = True
                return torch.tensor(
                    [[random.randrange(self.action_size)]],
                    device=self.device,
                    dtype=torch.long,
                )

        if self.evaluation == True:
            self.is_random = False
            self.target_net.eval()
            return self.target_net(state).max(1)[1].view(1, 1)

    def trainModel(self, length_memory, start_length):
        # no train
        if length_memory < start_length:
            return

        # train
        transitions = self.memory.sample(self.batch_size)
        mini_batch = self.transition(*zip(*transitions))

        state_batch = torch.cat([s.to(self.device) for s in mini_batch.state], dim=0)
        action_batch = torch.cat([a.to(self.device) for a in mini_batch.action], dim=0)
        reward_batch = torch.cat([r.to(self.device) for r in mini_batch.reward], dim=0)

        non_final_mask = torch.tensor(
            tuple(map(lambda s: s is not None, mini_batch.next_state)),
            dtype=torch.bool,
        ).to(self.device)
        non_final_next_states = torch.cat(
            [s.to(self.device) for s in mini_batch.next_state if s is not None]
        ).to(self.device)

        self.policy_net.train()
        action_value = (
            self.policy_net(state_batch).gather(1, action_batch).to(self.device)
        )
        # [64, 5] -> [64, 1] action 배치가 인덱스이므로 5개중에서 선택한걸 고름.

        self.target_net.eval()
        next_state_value = torch.zeros(self.batch_size, device=self.device)  # [64]
        next_state_value[non_final_mask] = self.target_net(non_final_next_states).max(
            1
        )[0]
        # max로 최대의 보상 [64, 5] -> [64]

        expected_q_value = (next_state_value * self.GAMMA) + reward_batch

        criterian = nn.SmoothL1Loss()
        loss = criterian(input=action_value, target=expected_q_value.unsqueeze(1))
        # unsqeeze -> [64, 1]

        self.optimizer.zero_grad()  # 기울기 초기화
        loss.backward()  # 역전파 계산
        # torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 120)
        self.optimizer.step()  # 가중치 업데이트

        self.loss_item = loss.item()


class GAIL:
    def __init__(self, state_dim, action_dim, discrete):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.pi = PolicyNetwork(state_dim, action_dim, discrete).to(self.device)
        self.v = ValueNetwork(state_dim).to(self.device)
        self.d = Discriminator(state_dim, action_dim, discrete).to(self.device)
        self.opt_d = torch.optim.Adam(self.d.parameters())

        self.demo_epochs = 2  # use to return mean of rewards of demos
        self.steps = 200

        self.env = Env_GAIL(action_dim)

        self.gae_gamma = 0.99
        self.gae_lambda = 0.99
        self.eps = 1e-2
        self.cg_damping = 1e-1
        self.max_kl = 1e-2
        self.lambda_ = 1e-3

    def getAction(self, state, global_step, load_episode):
        self.pi.eval()
        state = torch.tensor(state, dtype=torch.float32).to(self.device)
        distb = self.pi(state)
        action = distb.sample().detach().cpu().numpy()

        return action


def main():
    ## ROS node init
    rospy.init_node("gail_tb3_stage_4")
    pub_result = rospy.Publisher("result", Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher("get_action", Float32MultiArray, queue_size=5)

    result = Float32MultiArray()
    get_action = Float32MultiArray()

    state_size = 30
    action_size = 5
    expert = Expert(state_size, action_size)
    load_episode = 0
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()
    ## 출력용 변수
    expert.loss_item = 0
    ## 내부 계산용 변수
    score = 0
    goal = False
    end_step = 150  ## 150 => 30s
    is_random = 0

    ## 모델을 불러온다.

    if expert.load_dqn and expert.evaluation == False:
        folder = os.path.dirname(os.path.realpath(__file__))
        load_path = folder + "/stage_1150.tar"
        checkpoint = torch.load(load_path)
        load_episode = checkpoint["epoches"]
        expert.policy_net.load_state_dict(checkpoint["policy_net"])
        expert.target_net.load_state_dict(checkpoint["target_net"])
        expert.optimizer.load_state_dict(checkpoint["optimizer"])
        # score = checkpoint["score"]
        expert.memory.load("memory{}".format(load_episode))

        print("[Learning] torch load success @episode {}".format(load_episode))

    if expert.evaluation == True:
        folder = os.path.dirname(os.path.realpath(__file__))
        checkpoint = torch.load(folder + "/" + expert.target_ckpt)
        expert.target_net.load_state_dict(checkpoint)
        # expert.memory.load(folder + "/" + expert.target_mem) # no need
        gail = GAIL(state_dim=30, action_dim=2, discrete=False)

        exp_rwd_iter = []

        exp_obs = []
        exp_acts = []

    ## 총 훈련 횟수를 정한다.
    epoches = load_episode + expert.epoches
    if expert.evaluation:
        epoches = gail.demo_epochs

    ## main loop
    for epoch in range(load_episode + 1, epoches + 1):
        time_out = False
        truncated = False
        goal = False

        if expert.evaluation == True:
            ep_obs = []
            ep_rwds = []

        state = expert.env.reset()
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(expert.device)

        for local_step in range(expert.steps):
            action = expert.getAction(state, global_step, load_episode)
            if expert.evaluation == True:
                ep_obs.append(state.cpu().detach().numpy().squeeze())
                exp_obs.append(state.cpu().detach().numpy().squeeze())
                exp_acts.append(action.cpu().detach().numpy().squeeze())
            observation, reward, truncated = expert.env.step(action)
            if expert.evaluation == True:
                ep_rwds.append(reward)
            reward = torch.tensor([reward], device=expert.device)

            # print(
            #     "Steps: {:4d} {:3d} ({:6d}) Reward: {:.2f}({}) Memory: {:4d}".format(
            #         epoch,
            #         end_step - local_step,
            #         global_step,
            #         reward.cpu().detach().numpy().squeeze(),
            #         action.cpu().detach().numpy().squeeze(),
            #         len(expert.memory),
            #     )
            # )

            if reward >= 100:  ## Goal
                goal = True
            if local_step > end_step:  ## truncated for timeout.
                time_out = True
                truncated = True

            if goal or truncated:
                next_state = None
            else:
                next_state = torch.tensor(
                    observation, dtype=torch.float32, device=expert.device
                ).unsqueeze(0)
            expert.memory.push(state, action, reward, next_state)

            state = next_state

            ## 학습
            if expert.evaluation == False:
                expert.trainModel(len(expert.memory), expert.train_start)
                target_net_state_dict = expert.target_net.state_dict()
                policy_net_state_dict = expert.policy_net.state_dict()
                for key in policy_net_state_dict:
                    target_net_state_dict[key] = policy_net_state_dict[
                        key
                    ] * expert.TAU + target_net_state_dict[key] * (1 - expert.TAU)
                expert.target_net.load_state_dict(target_net_state_dict)

            score += reward
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)

            global_step += 1

            ## 종료조건 3가지(1. 목표지점 도달, 2. 충돌, 3. 시간초과)
            if goal:
                print(
                    "[DQN] Goal Reached. @step {} ({}/{})".format(
                        local_step, epoch, epoches
                    )
                )

                scores.append(score)
                episodes.append(epoch)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
                break

            if truncated:
                if time_out:
                    print("[DQN] Time out. @step {}".format(local_step))
                else:
                    print("[DQN] Collision. @step {}".format(local_step))

                scores.append(score)
                episodes.append(epoch)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                param_keys = ["epsilon"]
                param_values = [expert.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break

        if expert.evaluation == True:
            print(
                "Epoch: {:2d} Reward: {:.2f}".format(
                    epoch, score.cpu().detach().numpy().squeeze()
                )
            )
            exp_rwd_iter.append(ep_rwds)
            ep_obs = torch.tensor(np.array(ep_obs), device=gail.device)
            ep_rwds = torch.tensor(ep_rwds, device=gail.device)

    print("[DQN] Final score: {}".format(score.cpu().detach().numpy().squeeze()))

    ## 폴더
    index = time.localtime()
    index = "{:02d}_{:02d}_{:02d}_{:02d}".format(
        index.tm_mon, index.tm_mday, index.tm_hour, index.tm_min
    )

    # model
    folder = os.path.dirname(os.path.realpath(__file__))
    folder = folder.replace("gail_tb3/node", "gail_tb3/ckpt/{}".format(index))

    # FILE GENERATION
    if not os.path.exists(folder):
        os.makedirs(folder)

    if expert.evaluation == False:
        torch.save(expert.target_net.state_dict(), folder + "/target_net.ckpt")
        torch.save(
            {
                "score": score,
                "policy_net": expert.policy_net.state_dict(),
                "target_net": expert.target_net.state_dict(),
                "optimizer": expert.optimizer.state_dict(),
                "epoches": epoches,
            },
            folder + "/stage_{}.tar".format(epoch),
        )
        # torch.save(expert.policy_net.state_dict(), folder + "/policy_net.ckpt")
        # torch.save(expert.optimizer.state_dict(), folder + "/optimizer.ckpt")
        # 저장 memory
        expert.memory.save(folder, epoch)

    if expert.evaluation == False:
        print("DQN Learning Finished")
        rospy.spin()

    # GAIL
    for _ in range(len(exp_rwd_iter)):
        mean = +np.mean(exp_rwd_iter[_])
    exp_rwd_mean = mean / len(exp_rwd_iter)
    print("Expert Reward Mean: {}".format(exp_rwd_mean))
    # print("Expert Reward Iterations:")
    # for reward in exp_rwd_iter:
    #     print(f"  {reward}")
    # print()

    # expert data
    exp_obs = np.array(exp_obs)
    exp_obs = torch.tensor(exp_obs, device=gail.device)
    exp_acts = np.array(exp_acts)
    exp_acts = torch.tensor(exp_acts, device=gail.device)

    # train
    print("[GAIL] Novice trajectory generation started")
    rwd_iter_means = []
    for i in range(gail.demo_epochs):
        rwd_iter = []

        obs = []
        acts = []
        rets = []
        advs = []
        gms = []

        steps = 0
        print("test")
        while steps < gail.steps:
            ep_obs = []
            ep_acts = []
            ep_rwds = []
            ep_costs = []
            ep_disc_costs = []
            ep_gms = []
            ep_lmbs = []

            t = 0
            done = False

            ob = gail.env.reset()
            print("test")
            ob = torch.tensor(ob, dtype=torch.float32).to(gail.device)
            for step in range(gail.steps):
                act = gail.getAction(state, global_step, load_episode)

                ep_obs.append(state.cpu().detach().numpy().squeeze())
                obs.append(state.cpu().detach().numpy().squeeze())

                ep_acts.append(act.cpu().detach().numpy().squeeze())
                acts.append(act.cpu().detach().numpy().squeeze())

                observation, rwd, truncated = gail.env.step(act)

                ep_rwds.append(rwd)
                ep_gms.append(gail.gae_gamma**t)
                ep_lmbs.append(gail.gae_lambda**t)

                t += 1
                steps += 1

            rwd_iter.append(np.sum(ep_rwds))

            # calculate returns
            ep_obs = torch.tensor(np.array(ep_obs), device=gail.device)
            ep_acts = torch.tensor(np.array(ep_acts), device=gail.device)
            ep_rwds = torch.tensor(ep_rwds, device=gail.device)
            ep_gms = torch.tensor(ep_gms, device=gail.device)
            ep_lmbs = torch.tensor(ep_lmbs, device=gail.device)

            ep_costs = (-1) * torch.log(gail.d(ep_obs, ep_acts)).squeeze().detach()
            ep_disc_costs = ep_gms * ep_costs

            ep_disc_rets = torch.tensor(
                [sum(ep_disc_costs[i:]) for i in range(t)],
                dtype=torch.float32,
                device="cuda:0",
            )
            ep_rets = ep_disc_rets / ep_gms

            rets.append(ep_rets)

            # advantage estimation
            gail.v.eval()
            curr_vals = gail.v(ep_obs).detach()
            next_vals = torch.cat(
                (
                    gail.v(ep_obs)[1:],
                    torch.tensor([[0.0]], dtype=torch.float32, device="cuda:0"),
                )
            ).detach()
            ep_deltas = ep_costs.unsqueeze(-1) + gail.gae_gamma * next_vals - curr_vals

            ep_advs = torch.tensor(
                [
                    ((ep_gms * ep_lmbs)[: t - j].unsqueeze(-1) * ep_deltas[j:]).sum()
                    for j in range(t)
                ],
                dtype=torch.float32,
            )

            advs.append(ep_advs)

            gms.append(ep_gms)

        for _ in range(len(rwd_iter)):
            mean = +np.mean(rwd_iter[_])
            mean = mean / len(rwd_iter)
        rwd_iter_means.append(mean)
        print("[GAIL] Train: {} Reward Mean: {}".format(i + 1, mean))

        obs = torch.tensor(np.array(obs), device="cuda:0")
        acts = torch.tensor(np.array(acts), device="cuda:0")
        rets = torch.cat(rets)
        advs = torch.cat(advs)
        gms = torch.cat(gms)

        advs = ((advs - advs.mean()) / advs.std()).to(device)

        # train discriminator
        gail.d.train()
        exp_scores = gail.d.get_logits(exp_obs, exp_acts)
        nov_scores = gail.d.get_logits(obs, acts)

        gail.opt_d.zero_grad()
        loss = torch.nn.functional.binary_cross_entropy_with_logits(
            exp_scores, torch.zeros_like(exp_scores)
        ) + torch.nn.functional.binary_cross_entropy_with_logits(
            nov_scores, torch.ones_like(nov_scores)
        )
        loss.backward()
        gail.opt_d.step()

        # train value function(이것도 backprop을 풀어쓴거)
        gail.v.train()
        old_params = get_flat_params(gail.v).detach()
        # value function의 모든 파라미터를 가져옴.
        old_v = gail.v(obs).detach()
        # 업데이트 하기 전에 value function을 한번 통과시켜 state value를 저장해둠.

        def constraint():  # value fn.의 gradient를 구함.
            return ((old_v - gail.v(obs)) ** 2).mean()

        grad_diff = get_flat_grads(constraint(), gail.v)

        def Hv(v):
            hessian = get_flat_grads(torch.dot(grad_diff, v), gail.v).detach()

            return hessian

        g = get_flat_grads(
            ((-1) * (gail.v(obs).squeeze() - rets) ** 2).mean(), gail.v
        ).detach()  # V - GAE? or V - advantage, 이를 v와 비교하여 gradient를 구함.
        s = conjugate_gradient(Hv, g).detach()

        Hs = Hv(s).detach()
        alpha = torch.sqrt(2 * gail.eps / torch.dot(s, Hs))

        new_params = old_params + alpha * s

        set_params(gail.v, new_params)

        # train policy
        gail.pi.train()
        old_params = get_flat_params(gail.pi).detach()
        old_distb = gail.pi(obs)

        def L():
            distb = gail.pi(obs)

            return (  # TRPO의 loss
                advs  # advantage 여기 쓰임
                * torch.exp(
                    distb.log_prob(acts) - old_distb.log_prob(acts).detach(),
                )
            ).mean()

        def kld():
            distb = gail.pi(obs)

            if gail.discrete:
                old_p = old_distb.probs.detach()
                p = distb.probs

                return (old_p * (torch.log(old_p) - torch.log(p))).sum(-1).mean()

            else:
                old_mean = old_distb.mean.detach()
                old_cov = old_distb.covariance_matrix.sum(-1).detach()
                mean = distb.mean
                cov = distb.covariance_matrix.sum(-1)

                return (0.5) * (
                    (old_cov / cov).sum(-1)
                    + (((old_mean - mean) ** 2) / cov).sum(-1)
                    - gail.action_dim
                    + torch.log(cov).sum(-1)
                    - torch.log(old_cov).sum(-1)
                ).mean()

        grad_kld_old_param = get_flat_grads(kld(), gail.pi)

        def Hv(v):
            hessian = get_flat_grads(torch.dot(grad_kld_old_param, v), gail.pi).detach()

            return hessian + gail.cg_damping * v

        g = get_flat_grads(L(), gail.pi).detach()

        s = conjugate_gradient(Hv, g).detach()
        Hs = Hv(s).detach()

        new_params = rescale_and_linesearch(
            g, s, Hs, gail.max_kl, L, kld, old_params, gail.pi
        )

        disc_causal_entropy = ((-1) * gms * gail.pi(obs).log_prob(acts)).mean()
        grad_disc_causal_entropy = get_flat_grads(disc_causal_entropy, gail.pi)
        new_params += gail.lambda_ * grad_disc_causal_entropy

        set_params(gail.pi, new_params)

    print("GAIL Training Finished")

    results = exp_rwd_mean, rwd_iter_means
    print("exp_rwd_mean: ", exp_rwd_mean, "rwd_iter_means: ", rwd_iter_means)

    torch.save(gail.pi.state_dict(), folder + "/gail_pi.ckpt")
    torch.save(gail.v.state_dict(), folder + "/gail_v.ckpt")
    torch.save(gail.d.state_dict(), folder + "/gail_d.ckpt")


if __name__ == "__main__":
    main()
