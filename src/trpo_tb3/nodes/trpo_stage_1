#!/usr/bin/python3
import rospy
from std_msgs.msg import Float32MultiArray, String
import os
import numpy as np
import random
import time
import sys
from collections import deque, namedtuple

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn import Module

from src.model.trpo_env_1 import Env
from src.model.trpo_model import PolicyNetwork, ValueNetwork
from src.model.funcs import (
    get_flat_grads,
    get_flat_params,
    set_params,
    conjugate_gradient,
    rescale_and_linesearch,
)

# from src.trpo_memory import ReplayMemory
# from src.trpo_saving import SaveData

# Transition = namedtuple("Transition", ("state", "action", "reward", "next_state"))


class TRPO(Module):
    def __init__(self, state_dim, action_dim, discrete=False, stage=1) -> None:
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.discrete = discrete
        self.use_baseline = True

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.pi = PolicyNetwork(self.state_dim, self.action_dim, self.discrete)
        if self.use_baseline:
            self.v = ValueNetwork(self.state_dim)

        self.env = Env(action_dim)
        # self.save = SaveData(stage)

    def get_networks(self):
        if self.use_baseline:
            return [self.pi, self.v]
        else:
            return [self.pi]

    def act(self, state):
        self.pi.eval()
        state = torch.tensor(state, dtype=torch.float32).to(self.device)
        # print()
        # print(next(self.pi.parameters()).device)
        # print(state.device)
        # print()
        distb = self.pi(state)
        action = distb.sample().detach().cpu().numpy()

        return action

    def end_of_episode(self, episode, local_step, truncated, rwd):
        done = False
        goal = True if rwd == 1000 else False
        time_out = True if rwd == -100 else False

        # 종료조건 3가지(1. 목표지점 도달, 2. 충돌, 3. 시간초과)
        if goal:
            print("[Learning] Goal Reached. @step ", local_step)
            done = True

        if truncated:
            print("[Learning] Collision. @step ", local_step)
            done = True

        if time_out:
            print("[Learning] Time out. @step ", local_step)
            done = True

        return done

    def train(self):
        lr = 1e-3
        num_iters = 5000  # Episode
        num_steps_per_iter = 128  # max_iter per episode
        horizon = None
        discount = 0.99
        max_kl = 0.01
        cg_damping = 0.1
        normalize_return = True
        use_baseline = True

        if use_baseline:
            opt_v = torch.optim.Adam(self.v.parameters(), lr)

        rwd_iter_means = []
        for i in range(num_iters):
            rwd_iter = []

            obs = []
            acts = []
            rets = []
            disc = []

            steps = 0
            while (
                steps < num_steps_per_iter
            ):  # step does not go 0 when episode ends early
                ep_rwds = []
                ep_disc_rwds = []
                ep_disc = []

                t = 0
                done = False

                ob = self.env.reset()

                while not done and steps < num_steps_per_iter:
                    act = self.act(ob)

                    obs.append(ob)
                    acts.append(act)
                    ob, rwd, truncated = self.env.step(act, steps, num_steps_per_iter)

                    done = self.end_of_episode(i, t, truncated, rwd)

                    ep_rwds.append(rwd)
                    ep_disc_rwds.append(rwd * (discount**t))
                    ep_disc.append(discount**t)

                    t += 1
                    steps += 1

                    if horizon is not None:
                        if t >= horizon:
                            done = True
                            break

                    print("\nsteps: ", steps, "reward: ", rwd)

                ep_disc = torch.tensor(ep_disc, dtype=torch.float32)

                ep_disc_rets = torch.tensor(
                    [sum(ep_disc_rwds[i:]) for i in range(t)], dtype=torch.float32
                )
                ep_rets = ep_disc_rets / ep_disc

                rets.append(ep_rets)
                disc.append(ep_disc)

                if done:
                    rwd_iter.append(np.sum(ep_rwds))

            rwd_iter_means.append(np.mean(rwd_iter))
            print("Iterations: {},   Reward Mean: {}".format(i + 1, np.mean(rwd_iter)))

            obs = torch.tensor(np.array(obs), dtype=torch.float32).to(self.device)
            acts = torch.tensor(np.array(acts), dtype=torch.float32).to(self.device)
            rets = torch.cat(rets).to(self.device)
            disc = torch.cat(disc).to(self.device)

            if normalize_return:
                rets = (rets - rets.mean()) / rets.std()

            if use_baseline:  # Policy Based Optimization
                self.v.eval()
                # print(next(self.v.parameters()).device)
                # print(obs.device)
                delta = (rets - self.v(obs).squeeze()).detach()

                self.v.train()

                opt_v.zero_grad()
                loss = (-1) * disc * delta * self.v(obs).squeeze()
                loss.mean().backward()
                opt_v.step()

            self.pi.train()
            old_params = get_flat_params(self.pi).detach()
            old_distb = self.pi(obs)

            def L():
                distb = self.pi(obs)

                if use_baseline:
                    return (
                        disc
                        * delta
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()
                else:
                    return (
                        disc
                        * rets
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()

            def kld():
                distb = self.pi(obs)

                if self.discrete:
                    old_p = old_distb.probs.detach()
                    p = distb.probs

                    return (old_p * (torch.log(old_p) - torch.log(p))).sum(-1).mean()

                else:
                    old_mean = old_distb.mean.detach()
                    old_cov = old_distb.covariance_matrix.sum(-1).detach()
                    mean = distb.mean
                    cov = distb.covariance_matrix.sum(-1)

                    return (0.5) * (
                        (old_cov / cov).sum(-1)
                        + (((old_mean - mean) ** 2) / cov).sum(-1)
                        - self.action_dim
                        + torch.log(cov).sum(-1)
                        - torch.log(old_cov).sum(-1)
                    ).mean()

            grad_kld_old_param = get_flat_grads(kld(), self.pi)

            def Hv(v):
                hessian = get_flat_grads(
                    torch.dot(grad_kld_old_param, v), self.pi
                ).detach()

                return hessian + cg_damping * v

            g = get_flat_grads(L(), self.pi).detach()

            s = conjugate_gradient(Hv, g).detach()
            Hs = Hv(s).detach()

            new_params = rescale_and_linesearch(
                g, s, Hs, max_kl, L, kld, old_params, self.pi
            )

            set_params(self.pi, new_params)

        # performance list : obs, acts, ep_rets, ep_disc_rwds, ep_disc, rwd_iter
        return rwd_iter_means


class UTILS:
    def __init__(self) -> None:
        self.ckpt_path = os.path.dirname(os.path.realpath(__file__))
        self.stage = rospy.get_param("/stage_number")

    def save_model(self, model):
        current_time = time.localtime()
        year = current_time.tm_year
        month = current_time.tm_mon
        day = current_time.tm_mday
        hour = current_time.tm_hour
        minute = current_time.tm_min

        ckpt_path = self.ckpt_path
        ckpt_path = ckpt_path.replace(
            "nodes",
            "ckpt",
        )
        ckpt_path = ckpt_path + "/stage_{}/{}_{}_{}".format(
            self.stage, year, month, day
        )
        try:
            if not os.path.exists(ckpt_path):
                os.makedirs(ckpt_path)
                print("\n Directory for save is created!!")
                print("\n Directory: ", ckpt_path)
            torch.save(
                model.pi.state_dict(),
                os.path.join(ckpt_path, "policy_{}_{}.ckpt").format(hour, minute),
            )
            torch.save(
                model.v.state_dict(),
                os.path.join(ckpt_path, "value_{}_{}.ckpt".format(hour, minute)),
            )
        except:
            print("\n Error in saving model!!")


def main():
    # (ROS node init) trpo_stage_1
    stage = rospy.get_param("/stage_number")
    node_name = "trpo_stage_{stage}"
    rospy.init_node(node_name)
    # (ROS pub) result
    # pub_result = rospy.Publisher("result", Float32MultiArray, queue_size=5)
    # result = Float32MultiArray()
    # (ROS pub) get_action
    pub_get_action = rospy.Publisher("get_action", Float32MultiArray, queue_size=5)
    get_action = Float32MultiArray()

    # (Python) agent init
    state_size = 28
    action_size = 2
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = TRPO(state_size, action_size, discrete=False, stage=stage).to(device)
    utils = UTILS()
    # utils.save_model(agent)
    # (Python) learning start
    tic = time.time()
    rwd_iter_means = agent.train()
    # scores, episodes = [], []
    # global_step = 0
    # score = 0
    # goal = False
    # end_step = 150  ## 150 => 30s
    # is_random = 0

    ## 학습을 시작한다.
    ####### TO DO
    # ## 모델을 불러온다.
    # if agent.load_episode != 0:
    #     # agent.loadModel(agent.load_episode, agent.load_file)
    #     (
    #         agent.load_episode,
    #         agent.policy_net,
    #         agent.optimizer,
    #         score,
    #     ) = agent.save_data.loadModel(
    #         agent.load_episode,
    #         agent.load_file,
    #         agent.policy_net,
    #         agent.optimizer,
    #         agent.evaluation,
    #     )
    # if agent.load_memory:
    #     agent.memory.load(agent.memory_file)
    ## 저장할 폴더를 만든다.
    # agent.save_data.makeDir()

    ## 총 훈련 횟수를 정한다.
    # agent.EPISODE = agent.load_episode + agent.EPISODE
    # # agent.memory.push(state, action, reward, next_state)
    # # 저장할 메모리 준비
    # # if not global_step % 5000:
    #     # agent.memory.save(agent.save_data.memorySavedAt(), e)

    #     # 성능을 txt에 저장
    #     agent.save_data.recordPerformance(
    #         e,
    #         score,
    #         len(agent.memory),
    #         agent.epsilon,
    #         agent.loss_item,
    #         h,
    #         m,
    #         s,
    #         start_time,
    #         local_step,
    #     )
    #     # 모델을 저장
    #     if (e % 100) == 0:
    #         agent.save_data.saveModel(
    #             model=agent.policy_net,
    #             episode=e,
    #             optimizer=agent.optimizer,
    #             score=score,
    #         )
    ### TO DO
    # rospy.loginfo("[Learning] Start!!")

    print("[Learning] Finish!")
    print("[Learning] Mean of rewards: ")
    print(rwd_iter_means)
    # rospy.loginfo("[Notice] Final score: %d, loss: %d", score, agent.loss_item)

    m, s = divmod(int(time.time() - tic), 60)
    h, m = divmod(m, 60)
    print("[Learning] Time consumed: %d h %d m %d s", h, m, s)

    # agent.save_data.recordPerformance(
    #     e,
    #     score,
    #     len(agent.memory),
    #     agent.epsilon,
    #     agent.loss_item,
    #     h,
    #     m,
    #     s,
    #     start_time,
    #     local_step,
    # )
    # agent.save_data.saveModel(
    #     model=agent.policy_net, episode=e, optimizer=agent.optimizer, score=score
    # )
    # agent.memory.save(agent.save_data.memorySavedAt(), e)

    utils.save_model(agent)

    rospy.spin()


if __name__ == "__main__":
    main()
