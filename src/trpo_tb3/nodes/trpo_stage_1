#!/usr/bin/python3

import rospy
import os
import numpy as np
import random
import time
import sys

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque, namedtuple

from std_msgs.msg import Float32MultiArray
from src.trpo_env_1 import Env
from src.trpo_model import *
from src.trpo_memory import ReplayMemory
from src.trpo_saving import SaveData

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

if torch.cuda.is_available():
    from torch.cuda import FloatTensor

    torch.set_default_tensor_type(torch.cuda.FloatTensor)
else:
    from torch import FloatTensor


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

Transition = namedtuple("Transition", ("state", "action", "reward", "next_state"))


class TRPO(Module):
    def __init__(self, state_dim, action_dim, discrete=False) -> None:
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.discrete = discrete
        self.use_baseline = True

        self.pi = PolicyNetwork(self.state_dim, self.action_dim, self.discrete)
        if self.use_baseline:
            self.v = ValueNetwork(self.state_dim)

    def get_networks(self):
        if self.use_baseline:
            return [self.pi, self.v]
        else:
            return [self.pi]

    def act(self, state):
        self.pi.eval()

        state = FloatTensor(state)
        distb = self.pi(state)

        action = distb.sample().detach().cpu().numpy()

        return action

    def train(self, env, render=False):
        lr = 1e-3
        num_iters = 200  # Episode
        num_steps_per_iter = 2000  # max_iter per episode
        horizon = None
        discount = 0.99
        max_kl = 0.01
        cg_damping = 0.1
        normalize_return = True
        use_baseline = True

        if use_baseline:
            opt_v = torch.optim.Adam(self.v.parameters(), lr)

        rwd_iter_means = []
        for i in range(num_iters):
            rwd_iter = []

            obs = []
            acts = []
            rets = []
            disc = []

            steps = 0
            while steps < num_steps_per_iter:
                ep_rwds = []
                ep_disc_rwds = []
                ep_disc = []

                t = 0
                done = False

                ob = env.reset()

                while not done and steps < num_steps_per_iter:
                    act = self.act(ob)

                    obs.append(ob)
                    acts.append(act)

                    if render:
                        env.render()
                    observation, rwd, truncated = env.step(act)

                    ep_rwds.append(rwd)
                    ep_disc_rwds.append(rwd * (discount**t))
                    ep_disc.append(discount**t)

                    t += 1
                    steps += 1

                    if horizon is not None:
                        if t >= horizon:
                            done = True
                            break

                ep_disc = FloatTensor(ep_disc)

                ep_disc_rets = FloatTensor([sum(ep_disc_rwds[i:]) for i in range(t)])
                ep_rets = ep_disc_rets / ep_disc

                rets.append(ep_rets)
                disc.append(ep_disc)

                if done:
                    rwd_iter.append(np.sum(ep_rwds))

            rwd_iter_means.append(np.mean(rwd_iter))
            print("Iterations: {},   Reward Mean: {}".format(i + 1, np.mean(rwd_iter)))

            obs = FloatTensor(np.array(obs))
            acts = FloatTensor(np.array(acts))
            rets = torch.cat(rets)
            disc = torch.cat(disc)

            if normalize_return:
                rets = (rets - rets.mean()) / rets.std()

            if use_baseline:
                self.v.eval()
                delta = (rets - self.v(obs).squeeze()).detach()

                self.v.train()

                opt_v.zero_grad()
                loss = (-1) * disc * delta * self.v(obs).squeeze()
                loss.mean().backward()
                opt_v.step()

            self.pi.train()
            old_params = UTILS.get_flat_params(self.pi).detach()
            old_distb = self.pi(obs)

            def L():
                distb = self.pi(obs)

                if use_baseline:
                    return (
                        disc
                        * delta
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()
                else:
                    return (
                        disc
                        * rets
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()

            def kld():
                distb = self.pi(obs)

                if self.discrete:
                    old_p = old_distb.probs.detach()
                    p = distb.probs

                    return (old_p * (torch.log(old_p) - torch.log(p))).sum(-1).mean()

                else:
                    old_mean = old_distb.mean.detach()
                    old_cov = old_distb.covariance_matrix.sum(-1).detach()
                    mean = distb.mean
                    cov = distb.covariance_matrix.sum(-1)

                    return (0.5) * (
                        (old_cov / cov).sum(-1)
                        + (((old_mean - mean) ** 2) / cov).sum(-1)
                        - self.action_dim
                        + torch.log(cov).sum(-1)
                        - torch.log(old_cov).sum(-1)
                    ).mean()

            grad_kld_old_param = UTILS.get_flat_grads(kld(), self.pi)

            def Hv(v):
                hessian = UTILS.get_flat_grads(
                    torch.dot(grad_kld_old_param, v), self.pi
                ).detach()

                return hessian + cg_damping * v

            g = UTILS.get_flat_grads(L(), self.pi).detach()

            s = UTILS.conjugate_gradient(Hv, g).detach()
            Hs = Hv(s).detach()

            new_params = UTILS.rescale_and_linesearch(
                g, s, Hs, max_kl, L, kld, old_params, self.pi
            )

            UTILS.set_params(self.pi, new_params)

        return rwd_iter_means


if __name__ == "__main__":
    # (ROS print) torch.cuda.is_available()
    rospy.loginfo("%s", torch.cuda.is_available())
    # (ROS node init) trpo_stage_1
    rospy.init_node("trpo_stage_1")
    # (ROS pub) result
    pub_result = rospy.Publisher("result", Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    # (ROS pub) get_action
    pub_get_action = rospy.Publisher("get_action", Float32MultiArray, queue_size=5)
    get_action = Float32MultiArray()

    # (Python) agent init
    state_size = 30
    action_size = 5
    agent = TRPO(state_size, action_size)
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()
    agent.loss_item = 0
    score = 0
    goal = False
    end_step = 150  ## 150 => 30s
    is_random = 0

    ## 모델을 불러온다.
    if agent.load_episode != 0:
        # agent.loadModel(agent.load_episode, agent.load_file)
        (
            agent.load_episode,
            agent.policy_net,
            agent.optimizer,
            score,
        ) = agent.save_data.loadModel(
            agent.load_episode,
            agent.load_file,
            agent.policy_net,
            agent.optimizer,
            agent.evaluation,
        )

    if agent.load_memory:
        agent.memory.load(agent.memory_file)

    ## 저장할 폴더를 만든다.
    agent.save_data.makeDir()

    ## 총 훈련 횟수를 정한다.
    agent.EPISODE = agent.load_episode + agent.EPISODE

    ## main loop
    for e in range(agent.load_episode + 1, agent.EPISODE + 1):
        time_out = False
        truncated = False
        goal = False

        state = agent.env.reset()
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)

        for local_step in range(agent.episode_step):
            action = agent.getAction(state, global_step)
            observation, reward, truncated = agent.env.step(action)
            reward = torch.tensor([reward], device=device)

            rospy.loginfo(
                "[Learning] step to end e%d: %d(%d), memory: %d, epsilon thres: %.2f, is_random: %s, action: %d, reward: %.3f, loss: %f",
                e,
                end_step - local_step,
                global_step,
                len(agent.memory),
                agent.epsilon_threshold,
                str(is_random),
                action,
                reward,
                agent.loss_item,
            )

            if reward >= 100:  ## Goal
                goal = True
            if local_step > end_step:  ## truncated for timeout.
                time_out = True
                truncated = True

            if goal or truncated:
                next_state = None
            else:
                next_state = torch.tensor(
                    observation, dtype=torch.float32, device=device
                ).unsqueeze(0)
            agent.memory.push(state, action, reward, next_state)
            state = next_state
            ## 학습
            agent.trainModel(len(agent.memory), agent.train_start)
            target_net_state_dict = agent.target_net.state_dict()
            policy_net_state_dict = agent.policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[
                    key
                ] * agent.TAU + target_net_state_dict[key] * (1 - agent.TAU)
            agent.target_net.load_state_dict(target_net_state_dict)

            # score += reward
            # get_action.data = [action, score, reward]
            # pub_get_action.publish(get_action)

            # 저장할 메모리 준비
            if not global_step % 5000:
                agent.memory.save(agent.save_data.memorySavedAt(), e)

            global_step += 1

            ## 종료조건 3가지(1. 목표지점 도달, 2. 충돌, 3. 시간초과)
            if goal:
                rospy.loginfo("[Learning] Goal Reached. @step %d", local_step)

                scores.append(score)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
                break

            if truncated:
                if time_out:
                    rospy.loginfo("[Learning] Time out. @step %d", local_step)
                else:
                    rospy.loginfo("[Learning] Collision. @step %d", local_step)

                scores.append(score)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                param_keys = ["epsilon"]
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break

        # 성능을 txt에 저장
        agent.save_data.recordPerformance(
            e,
            score,
            len(agent.memory),
            agent.epsilon,
            agent.loss_item,
            h,
            m,
            s,
            start_time,
            local_step,
        )
        # 모델을 저장
        if (e % 100) == 0:
            agent.save_data.saveModel(
                model=agent.policy_net,
                episode=e,
                optimizer=agent.optimizer,
                score=score,
            )

    rospy.loginfo("[Notice] Finish! Press any key to exit.")
    rospy.loginfo("[Notice] Final score: %d, loss: %d", score, agent.loss_item)

    m, s = divmod(int(time.time() - start_time), 60)
    h, m = divmod(m, 60)
    agent.save_data.recordPerformance(
        e,
        score,
        len(agent.memory),
        agent.epsilon,
        agent.loss_item,
        h,
        m,
        s,
        start_time,
        local_step,
    )
    agent.save_data.saveModel(
        model=agent.policy_net, episode=e, optimizer=agent.optimizer, score=score
    )
    agent.memory.save(agent.save_data.memorySavedAt(), e)
    rospy.spin()
