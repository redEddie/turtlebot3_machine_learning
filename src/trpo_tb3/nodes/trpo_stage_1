#!/usr/bin/python3

import rospy
import os
import numpy as np
import random
import time
import sys

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque, namedtuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn import Module

from std_msgs.msg import Float32MultiArray
from src.model.trpo_env_1 import Env
from src.model.trpo_model import PolicyNetwork, ValueNetwork, UTILS

# from src.trpo_memory import ReplayMemory
# from src.trpo_saving import SaveData


if torch.cuda.is_available():
    from torch.cuda import FloatTensor

    torch.set_default_dtype(torch.float32)
else:
    from torch import FloatTensor


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

Transition = namedtuple("Transition", ("state", "action", "reward", "next_state"))


def end_point(episode, local_step, truncated, rwd):
    done = False
    goal = True if rwd == 1000 else False
    time_out = True if local_step > end_step else False

    # 종료조건 3가지(1. 목표지점 도달, 2. 충돌, 3. 시간초과)
    if goal:
        rospy.loginfo("[Learning] Goal Reached. @step %d", local_step)
        done = True

        # scores.append(score)
        # episodes.append(e)
        # m, s = divmod(int(time.time() - start_time), 60)
        # h, m = divmod(m, 60)

    if truncated:
        rospy.loginfo("[Learning] Collision. @step %d", local_step)
        done = True

        # scores.append(score)
        # episodes.append(e)
        # m, s = divmod(int(time.time() - start_time), 60)
        # h, m = divmod(m, 60)

        # param_keys = ["epsilon"]
        # param_values = [agent.epsilon]
        # param_dictionary = dict(zip(param_keys, param_values))

    if time_out:
        rospy.loginfo("[Learning] Time out. @step %d", local_step)
        done = True

    return done


class TRPO(Module):
    def __init__(self, state_dim, action_dim, discrete=False, stage_number=1) -> None:
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.discrete = discrete
        self.use_baseline = True

        self.pi = PolicyNetwork(self.state_dim, self.action_dim, self.discrete)
        if self.use_baseline:
            self.v = ValueNetwork(self.state_dim)

        self.utils = UTILS()
        # self.save = SaveData(stage_number)

    def get_networks(self):
        if self.use_baseline:
            return [self.pi, self.v]
        else:
            return [self.pi]

    def act(self, state):
        self.pi.eval()

        state = FloatTensor(state)
        distb = self.pi(state)

        action = distb.sample().detach().cpu().numpy()

        return action

    def train(self, env):
        lr = 1e-3
        num_iters = 200  # Episode
        num_steps_per_iter = 2000  # max_iter per episode
        horizon = None
        discount = 0.99
        max_kl = 0.01
        cg_damping = 0.1
        normalize_return = True
        use_baseline = True

        if use_baseline:
            opt_v = torch.optim.Adam(self.v.parameters(), lr)

        rwd_iter_means = []
        for i in range(num_iters):
            rwd_iter = []

            obs = []
            acts = []
            rets = []
            disc = []

            steps = 0
            while steps < num_steps_per_iter:
                ep_rwds = []
                ep_disc_rwds = []
                ep_disc = []

                t = 0
                done = False

                ob = env.reset()

                while not done and steps < num_steps_per_iter:
                    act = self.act(ob)

                    obs.append(ob)
                    acts.append(act)

                    ob, rwd, truncated = env.step(act)

                    ep_rwds.append(rwd)
                    ep_disc_rwds.append(rwd * (discount**t))
                    ep_disc.append(discount**t)

                    t += 1
                    steps += 1

                    if horizon is not None:
                        if t >= horizon:
                            done = True
                            break

                    end_point(i, t, truncated, rwd)

                ep_disc = FloatTensor(ep_disc)

                ep_disc_rets = FloatTensor([sum(ep_disc_rwds[i:]) for i in range(t)])
                ep_rets = ep_disc_rets / ep_disc

                rets.append(ep_rets)
                disc.append(ep_disc)

                if done:
                    rwd_iter.append(np.sum(ep_rwds))

            rwd_iter_means.append(np.mean(rwd_iter))
            print("Iterations: {},   Reward Mean: {}".format(i + 1, np.mean(rwd_iter)))

            obs = FloatTensor(np.array(obs))
            acts = FloatTensor(np.array(acts))
            rets = torch.cat(rets)
            disc = torch.cat(disc)

            if normalize_return:
                rets = (rets - rets.mean()) / rets.std()

            if use_baseline:  # Policy Based Optimization
                self.v.eval()
                delta = (rets - self.v(obs).squeeze()).detach()

                self.v.train()

                opt_v.zero_grad()
                loss = (-1) * disc * delta * self.v(obs).squeeze()
                loss.mean().backward()
                opt_v.step()

            self.pi.train()
            old_params = self.utils.get_flat_params(self.pi).detach()
            old_distb = self.pi(obs)

            def L():
                distb = self.pi(obs)

                if use_baseline:
                    return (
                        disc
                        * delta
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()
                else:
                    return (
                        disc
                        * rets
                        * torch.exp(
                            distb.log_prob(acts) - old_distb.log_prob(acts).detach()
                        )
                    ).mean()

            def kld():
                distb = self.pi(obs)

                if self.discrete:
                    old_p = old_distb.probs.detach()
                    p = distb.probs

                    return (old_p * (torch.log(old_p) - torch.log(p))).sum(-1).mean()

                else:
                    old_mean = old_distb.mean.detach()
                    old_cov = old_distb.covariance_matrix.sum(-1).detach()
                    mean = distb.mean
                    cov = distb.covariance_matrix.sum(-1)

                    return (0.5) * (
                        (old_cov / cov).sum(-1)
                        + (((old_mean - mean) ** 2) / cov).sum(-1)
                        - self.action_dim
                        + torch.log(cov).sum(-1)
                        - torch.log(old_cov).sum(-1)
                    ).mean()

            grad_kld_old_param = self.utils.get_flat_grads(kld(), self.pi)

            def Hv(v):
                hessian = self.utils.get_flat_grads(
                    torch.dot(grad_kld_old_param, v), self.pi
                ).detach()

                return hessian + cg_damping * v

            g = self.utils.get_flat_grads(L(), self.pi).detach()

            s = self.utils.conjugate_gradient(Hv, g).detach()
            Hs = Hv(s).detach()

            new_params = self.utils.rescale_and_linesearch(
                g, s, Hs, max_kl, L, kld, old_params, self.pi
            )

            self.utils.set_params(self.pi, new_params)

        return rwd_iter_means


if __name__ == "__main__":
    stage_number = 1
    # (ROS print) torch.cuda.is_available()
    rospy.loginfo("%s", torch.cuda.is_available())
    # (ROS node init) trpo_stage_1
    node_name = "trpo_stage_{stage_number}"
    rospy.init_node(node_name)
    # (ROS pub) result
    pub_result = rospy.Publisher("result", Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    # (ROS pub) get_action
    pub_get_action = rospy.Publisher("get_action", Float32MultiArray, queue_size=5)
    get_action = Float32MultiArray()

    # (Python) agent init
    state_size = 24 + 7
    action_size = 2
    agent = TRPO(state_size, action_size, stage_number=1)
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()
    score = 0
    goal = False
    end_step = 150  ## 150 => 30s
    is_random = 0

    ## 학습을 시작한다.
    ####### TODO
    # ## 모델을 불러온다.
    # if agent.load_episode != 0:
    #     # agent.loadModel(agent.load_episode, agent.load_file)
    #     (
    #         agent.load_episode,
    #         agent.policy_net,
    #         agent.optimizer,
    #         score,
    #     ) = agent.save_data.loadModel(
    #         agent.load_episode,
    #         agent.load_file,
    #         agent.policy_net,
    #         agent.optimizer,
    #         agent.evaluation,
    #     )
    # if agent.load_memory:
    #     agent.memory.load(agent.memory_file)
    ## 저장할 폴더를 만든다.
    # agent.save_data.makeDir()

    ## 총 훈련 횟수를 정한다.
    # agent.EPISODE = agent.load_episode + agent.EPISODE
    # # agent.memory.push(state, action, reward, next_state)
    # # 저장할 메모리 준비
    # # if not global_step % 5000:
    #     # agent.memory.save(agent.save_data.memorySavedAt(), e)

    #     # 성능을 txt에 저장
    #     agent.save_data.recordPerformance(
    #         e,
    #         score,
    #         len(agent.memory),
    #         agent.epsilon,
    #         agent.loss_item,
    #         h,
    #         m,
    #         s,
    #         start_time,
    #         local_step,
    #     )
    #     # 모델을 저장
    #     if (e % 100) == 0:
    #         agent.save_data.saveModel(
    #             model=agent.policy_net,
    #             episode=e,
    #             optimizer=agent.optimizer,
    #             score=score,
    #         )
    ### TODO

    env = Env(action_size)
    rwd_iter_means = agent.train(env)

    rospy.loginfo("[Notice] Finish! Press any key to exit.")
    rospy.loginfo("[Notice] Mean of rewards: %f", rwd_iter_means)
    # rospy.loginfo("[Notice] Final score: %d, loss: %d", score, agent.loss_item)

    m, s = divmod(int(time.time() - start_time), 60)
    h, m = divmod(m, 60)
    rospy.loginfo("[Notice] Time: %d h %d m %d s", h, m, s)

    # agent.save_data.recordPerformance(
    #     e,
    #     score,
    #     len(agent.memory),
    #     agent.epsilon,
    #     agent.loss_item,
    #     h,
    #     m,
    #     s,
    #     start_time,
    #     local_step,
    # )
    # agent.save_data.saveModel(
    #     model=agent.policy_net, episode=e, optimizer=agent.optimizer, score=score
    # )
    # agent.memory.save(agent.save_data.memorySavedAt(), e)
    rospy.spin()
